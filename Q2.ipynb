{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "33cb2d5c54894b89922e117842584818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18c07dcbec8a4b52b732a41fa8a5af53",
              "IPY_MODEL_d95c093f69f344909a1cfb016b1ba0cc",
              "IPY_MODEL_a43446f73ff847ca885f95e8df259419"
            ],
            "layout": "IPY_MODEL_963387396f6d4ff4aaf87cd6bfd0f622"
          }
        },
        "18c07dcbec8a4b52b732a41fa8a5af53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d16bdbd527b34dfeb3b0437c94a63249",
            "placeholder": "​",
            "style": "IPY_MODEL_5fa7af1b6fe84512bacc01914ad7aa00",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "d95c093f69f344909a1cfb016b1ba0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_310ccf2593ff417dbb961ff87f736594",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_699071b756cf41729371b899a201ffbd",
            "value": 20
          }
        },
        "a43446f73ff847ca885f95e8df259419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b74fda7037a437780122f97fed64615",
            "placeholder": "​",
            "style": "IPY_MODEL_f1e1fc56bdc742218c676d11e7727150",
            "value": " 20/20 [00:03&lt;00:00,  7.75 examples/s]"
          }
        },
        "963387396f6d4ff4aaf87cd6bfd0f622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d16bdbd527b34dfeb3b0437c94a63249": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fa7af1b6fe84512bacc01914ad7aa00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "310ccf2593ff417dbb961ff87f736594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "699071b756cf41729371b899a201ffbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b74fda7037a437780122f97fed64615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e1fc56bdc742218c676d11e7727150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cLaSA91pLOcS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52cc2395-f046-4e81-89da-e6ce7efb2b2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face token: ··········\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# Prompt for the Hugging Face token securely\n",
        "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
        "\n",
        "# Login to Hugging Face Hub\n",
        "login(token=hf_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import wandb\n",
        "import getpass\n",
        "\n",
        "\n",
        "# Login to Weights & Biases\n",
        "wb_token = getpass.getpass(\"Enter your Weights & Biases API Key: \")\n",
        "wandb.login(key=wb_token)\n",
        "\n",
        "# Initialize W&B run\n",
        "run = wandb.init(\n",
        "    project=\"Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset\",\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "0wBS4akRLbk4",
        "outputId": "4dd88b05-3f20-4c3c-835e-aaaf65c262dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Weights & Biases API Key: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaadashraf12\u001b[0m (\u001b[33msaadashraf12-fast-nuces\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250313_195246-dbgsqn51</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/saadashraf12-fast-nuces/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/dbgsqn51?apiKey=d58371bfc968f8ecce7d3018490b7094129cf948' target=\"_blank\">wandering-meadow-3</a></strong> to <a href='https://wandb.ai/saadashraf12-fast-nuces/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset?apiKey=d58371bfc968f8ecce7d3018490b7094129cf948' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/saadashraf12-fast-nuces/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset?apiKey=d58371bfc968f8ecce7d3018490b7094129cf948' target=\"_blank\">https://wandb.ai/saadashraf12-fast-nuces/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset?apiKey=d58371bfc968f8ecce7d3018490b7094129cf948</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/saadashraf12-fast-nuces/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/dbgsqn51?apiKey=d58371bfc968f8ecce7d3018490b7094129cf948' target=\"_blank\">https://wandb.ai/saadashraf12-fast-nuces/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/dbgsqn51?apiKey=d58371bfc968f8ecce7d3018490b7094129cf948</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Do NOT share these links with anyone. They can be used to claim your runs."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unsloth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKuS1N5YAX9N",
        "outputId": "f01c1f8d-9dcd-4ecf-95ec-690be5dfafad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import SFTTrainer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "WOdcMNrHEaBI",
        "outputId": "6e4d2808-d9f8-47fe-cd98-319aed2fe7a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'SFTTrainer' from 'unsloth' (/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b64bc55bcc58>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'SFTTrainer' from 'unsloth' (/usr/local/lib/python3.11/dist-packages/unsloth/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = hf_token,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxhcvkMfLcyN",
        "outputId": "688ad33e-eeaf-4129-9fdc-7a468f9709a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.10: Fast Llama patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_style = \"\"\"Below is a math meme with an incorrect solution. Your task is to identify the error and provide a correct explanation.\n",
        "\n",
        "### Incorrect Meme:\n",
        "{}\n",
        "\n",
        "### Identified Error:\n",
        "{}\n",
        "\n",
        "### Fixed Explanation:\n",
        "{}\"\"\"\n"
      ],
      "metadata": {
        "id": "X92biCreLgqx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is set for inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def test_math_meme(meme_description):\n",
        "    input_text = prompt_style.format(meme_description, \"\", \"\")\n",
        "\n",
        "    inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=500,\n",
        "        use_cache=True,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Print full raw response for debugging\n",
        "    print(\"Raw Response:\\n\", response)\n",
        "\n",
        "    # Extract all sections labeled \"### Fixed Explanation:\"\n",
        "    explanations = response.split(\"### Fixed Explanation\")\n",
        "\n",
        "    # Filter out empty or placeholder explanations\n",
        "    valid_explanations = [exp.strip() for exp in explanations if exp.strip() and \"error\" not in exp.lower()]\n",
        "\n",
        "    if valid_explanations:\n",
        "        extracted_explanation = valid_explanations[-1]  # Take the last meaningful explanation\n",
        "        print(\"\\nExtracted Explanation:\\n\", extracted_explanation)\n",
        "    else:\n",
        "        print(\"\\nNo valid explanation detected.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "47A7WC4hL0-h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test case\n",
        "#test_meme = \"A meme stating that sin(90°) = 0\"\n",
        "#test_meme = \"A post claiming that log(1) = 1\"\n",
        "test_meme = \"A meme stating that 8 ÷ 2(2 + 2) = 1\"\n",
        "test_math_meme(test_meme)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQvwPdcqZ8GO",
        "outputId": "b7c408ef-9773-48d4-92b4-6bfd77e96305"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Response:\n",
            " Below is a math meme with an incorrect solution. Your task is to identify the error and provide a correct explanation.\n",
            "\n",
            "### Incorrect Meme:\n",
            "A meme stating that 8 ÷ 2(2 + 2) = 1\n",
            "\n",
            "### Identified Error:\n",
            "\n",
            "\n",
            "### Fixed Explanation:\n",
            "<Think here>\n",
            "\n",
            "Okay, so I came across this math meme, and at first glance, it seemed a bit confusing because of the way the numbers are arranged. The meme says 8 ÷ 2(2 + 2) = 1. Hmm, let me try to figure out what's going on here.\n",
            "\n",
            "Alright, starting with the expression: 8 ÷ 2(2 + 2). I remember that in math, parentheses mean you do what's inside them first. So, let me focus on the part inside the parentheses: 2 + 2. That's straightforward, right? 2 plus 2 is 4. So now the expression becomes 8 ÷ 2(4). \n",
            "\n",
            "Wait, hold on, it's 8 divided by 2 times 4. Hmm, I think I need to remember the order of operations here. I recall PEMDAS: Parentheses, Exponents, Multiplication and Division (from left to right). So after doing the parentheses, we have multiplication next. So, 2 times 4 is 8. That means the expression simplifies to 8 ÷ 8. \n",
            "\n",
            "Oh, 8 divided by 8 is 1. So, actually, the meme is correct? But why was it labeled as incorrect? Maybe I'm missing something. Let me double-check. \n",
            "\n",
            "Wait, maybe the way the meme is written is tripping me up. Is it 8 divided by [2 times (2 + 2)]? Yes, that's what I did. So 2 + 2 is 4, multiplied by 2 is 8, then 8 divided by 8 is 1. So it does equal 1. Hmm, maybe the meme was trying to trick me into thinking it's more complicated, but it's actually straightforward.\n",
            "\n",
            "Alternatively, maybe the original expression was misinterpreted without the parentheses. If it were 8 ÷ 2 × 2 + 2, that would be different because multiplication and addition are on the same level. So left to right, it would be 8 ÷ 2 = 4, then 4 × 2 = 8, then 8 + 2 = 10. But in the meme, the parentheses make it clear that it's 2 times (2 + 2), so the division is by 8, giving 1.\n",
            "\n",
            "Wait, so the error must be\n",
            "\n",
            "No valid explanation detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_prompt_style = \"\"\"Below is a math meme with an incorrect solution. Your task is to identify the error, explain it, and provide the correct answer.\n",
        "\n",
        "### Incorrect Meme:\n",
        "{}\n",
        "\n",
        "### Identified Error:\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "\n",
        "### Fixed Explanation:\n",
        "{}\"\"\"\n"
      ],
      "metadata": {
        "id": "d1EPHdjVL4QE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Ensure EOS token is appended\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    descriptions = examples[\"description\"]  # Incorrect meme\n",
        "    errors = examples[\"error\"]  # Identified mistake\n",
        "    explanations = examples[\"explanation\"]  # Corrected response\n",
        "\n",
        "    texts = [\n",
        "        train_prompt_style.format(desc, err, exp) + EOS_TOKEN\n",
        "        for desc, err, exp in zip(descriptions, errors, explanations)\n",
        "    ]\n",
        "\n",
        "    return texts  # ✅ Return a list instead of a dictionary\n"
      ],
      "metadata": {
        "id": "axe8lrkVL-k0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "dataset_path = \"/content/math_meme.jsonl\"\n",
        "\n",
        "# Read JSONL file into a list of dictionaries\n",
        "with open(dataset_path, \"r\") as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Convert list of dictionaries to a Hugging Face Dataset\n",
        "hf_dataset = Dataset.from_pandas(pd.DataFrame(dataset))\n"
      ],
      "metadata": {
        "id": "r_wvmEUQMBjH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Femud8cIMEhR",
        "outputId": "eab889ae-1b28-403e-cbf5-4e8761308e5d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.3.10 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported"
      ],
      "metadata": {
        "id": "RRnRgWKPFBYJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=hf_dataset,  # ✅ Now a Hugging Face Dataset\n",
        "    dataset_text_field=\"text\",\n",
        "    formatting_func=formatting_prompts_func,  # ✅ Ensures text formatting\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "33cb2d5c54894b89922e117842584818",
            "18c07dcbec8a4b52b732a41fa8a5af53",
            "d95c093f69f344909a1cfb016b1ba0cc",
            "a43446f73ff847ca885f95e8df259419",
            "963387396f6d4ff4aaf87cd6bfd0f622",
            "d16bdbd527b34dfeb3b0437c94a63249",
            "5fa7af1b6fe84512bacc01914ad7aa00",
            "310ccf2593ff417dbb961ff87f736594",
            "699071b756cf41729371b899a201ffbd",
            "9b74fda7037a437780122f97fed64615",
            "f1e1fc56bdc742218c676d11e7727150"
          ]
        },
        "id": "391w7EwdMNmI",
        "outputId": "a36e1da9-8eaa-45bd-9ab9-a75b11c5c740"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/20 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33cb2d5c54894b89922e117842584818"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "HYzmviTyMR3F",
        "outputId": "f13e5e40-be8d-40d3-a03e-a61c06485dec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 20 | Num Epochs = 12 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 41,943,040/4,670,623,744 (0.90% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 03:24, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.822100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.335000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.086400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.057400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.049500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.043900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where you want to save the model\n",
        "output_dir = \"my_trained_model\"\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {output_dir}\")\n"
      ],
      "metadata": {
        "id": "uVnqDjvvM2fo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "150b0c78-9203-484a-9ebd-655f75dba1f1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved to my_trained_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Free up GPU memory before loading the model\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "\n",
        "# Define model path\n",
        "output_dir = \"my_trained_model\"\n",
        "\n",
        "# Choose quantization level: 8-bit or 4-bit\n",
        "use_4bit = False  # Set to True if you need even lower memory usage\n",
        "\n",
        "# Configure quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    load_in_8bit=not use_4bit,  # Use 8-bit if not using 4-bit\n",
        "    bnb_4bit_compute_dtype=torch.float16 if use_4bit else None,\n",
        "    bnb_4bit_use_double_quant=True if use_4bit else False,\n",
        "    llm_int8_enable_fp32_cpu_offload=True if not use_4bit else False  # Offload FP32 to CPU in 8-bit mode\n",
        ")\n",
        "\n",
        "# Load the model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir, quantization_config=bnb_config).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Set model max sequence length manually if needed\n",
        "model.max_seq_length = 2048\n",
        "\n",
        "# Example inference function\n",
        "def generate_text(prompt, max_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=max_tokens,  # Limit token generation to save memory\n",
        "            do_sample=True,  # Set to False for deterministic output\n",
        "            temperature=0.3  # Adjust creativity\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test inference\n",
        "prompt = \"A social media post claiming that 0! = 0\"\n",
        "print(generate_text(prompt, max_tokens=100))\n"
      ],
      "metadata": {
        "id": "1hf-VJpiM345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d30bef-c15a-4cbc-98f8-d157ed72f93e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A social media post claiming that 0! = 0 is incorrect. Explain\n",
            "A social media post claiming that 0! = 0 is incorrect because 0! (zero factorial) is defined as 1, not 0. This definition is consistent with the factorial function, which for any positive integer n, n! = n × (n-1) × ... × 1. When n=0, the product is 1 by convention.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Free up GPU memory before loading the model\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_max_memory_allocated()\n",
        "\n",
        "# Define model path\n",
        "output_dir = \"my_trained_model\"\n",
        "\n",
        "# Choose quantization level: 8-bit or 4-bit\n",
        "use_4bit = False  # Set to True if you need even lower memory usage\n",
        "\n",
        "# Configure quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    load_in_8bit=not use_4bit,  # Use 8-bit if not using 4-bit\n",
        "    bnb_4bit_compute_dtype=torch.float16 if use_4bit else None,\n",
        "    bnb_4bit_use_double_quant=True if use_4bit else False,\n",
        "    llm_int8_enable_fp32_cpu_offload=True if not use_4bit else False  # Offload FP32 to CPU in 8-bit mode\n",
        ")\n",
        "\n",
        "# Load the model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir, quantization_config=bnb_config).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Set model max sequence length manually if needed\n",
        "model.max_seq_length = 2048\n",
        "\n",
        "# Example inference function\n",
        "def generate_text(prompt, max_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=max_tokens,  # Limit token generation to save memory\n",
        "            do_sample=True,  # Set to False for deterministic output\n",
        "            temperature=0.3  # Adjust creativity\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test inference\n",
        "prompt = \"A meme showing 9 - 3 ÷ 1/3 + 1 = ?\"\n",
        "print(generate_text(prompt, max_tokens=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tIjRfnDGr5B",
        "outputId": "4f271015-d37f-4529-8cb8-872735493cea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A meme showing 9 - 3 ÷ 1/3 + 1 = ? with an incorrect solution\n",
            "A social media post claiming that 9 - 3 ÷ 1/3 + 1 equals 10\n",
            "\n",
            "### Fixed Explanation:\n",
            "Correct calculation: 9 - 3 ÷ (1/3) + 1 = 9 - 9 + 1 = 1\n",
            "</think>\n",
            "\n",
            "### Fixed Explanation:\n",
            "Correct calculation: 9 - 3 ÷ (1/3) + 1 = 9 - 9 + 1 = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6vGwnriDHyZE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}